slow_conv2d_cpu" not implemented for 'Half'
when dtype=float16
apparently this is caused by not using GPU:
https://stackoverflow.com/questions/74725439/runtimeerror-slow-conv2d-cpu-not-implemented-for-half
solution: do BLIP but not with fp16

Dec 12
RuntimeError: "LayerNormKernelImpl" not implemented for 'Half'

possibly caused by use mixed precision when it should be full precision

problem:
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

solution:
might be a problem with adamW optimizer
/home/jlb638/.conda/envs/clip/lib/python3.11/site-packages/trl/trainer/ddpo_trainer.py L398 
change to adam optimizer

12/13

DDPO with creativity: 

['ddpo_test.py', 'no']
images.shape  torch.Size([1, 3, 512, 512])
images.shape  torch.Size([1, 3, 512, 512])
line 233 rewards  (tensor([-0.9994], device='cuda:0'), tensor([-0.9993], device='cuda:0'))
line 244 rewards  [-0.9993682 -0.9993267]
line 263 advantages  [-0.9995181  0.9995181]
line 272 samples["advantages"] tensor([-0.9995,  0.9995], device='cuda:0')
line 273 samples["advantages"].requires_grad False
line 521 sample["advantages"] tensor([0.9995], device='cuda:0')
line 343 advantages False torch.Size([1])
line 377 advantages False torch.Size([1])
line 397 advantages.requires_grad False
line 398 advantages.size() torch.Size([1])
line 385 loss.requires_grad False
line 386 loss.size() torch.Size([])
line 537 loss.requires_grad False
line 538 loss.shape torch.Size([])

DDPO with aesthetic:
['ddpo_test.py', 'no', 'aesthetic']
line 233 rewards  (tensor([6.6416], device='cuda:0'), tensor([4.2934], device='cuda:0'))
line 244 rewards  [6.6416025 4.293413 ]
line 263 advantages  [ 1. -1.]
line 272 samples["advantages"] tensor([ 1., -1.], device='cuda:0')
line 273 samples["advantages"].requires_grad False
line 521 sample["advantages"] tensor([-1.], device='cuda:0')
line 343 advantages False torch.Size([1])
line 377 advantages False torch.Size([1])
line 397 advantages.requires_grad False
line 398 advantages.size() torch.Size([1])
line 385 loss.requires_grad False
line 386 loss.size() torch.Size([])
line 537 loss.requires_grad False
line 538 loss.shape torch.Size([])

maybe try running it on ONE GPU???

Soluton:
unsqueeze line 412???

images.shape  torch.Size([1, 3, 512, 512])
images.shape  torch.Size([1, 3, 512, 512])
line 233 rewards  (tensor([-9.9937], device='cuda:0'), tensor([-9.9933], device='cuda:0'))
line 244 rewards  [-9.993682 -9.993267]
line 263 advantages  [-0.9976504  1.0022479]
line 272 samples["advantages"] tensor([-0.9977,  1.0022], device='cuda:0')
line 273 samples["advantages"].requires_grad False
line 521 sample["advantages"] tensor([1.0022], device='cuda:0')
line 343 advantages False torch.Size([1])
line 377 advantages False torch.Size([1])
line 400 advantages.requires_grad False
line 401 advantages.size() torch.Size([1])
line 408 unclipped_loss  tensor([-1.0022], device='cuda:0')
line 409 clipped_loss tensor([-1.0022], device='cuda:0')
line 410 torch.maximum(unclipped_loss, clipped_loss) tensor([-1.0022], device='cuda:0')
line 411 torch.mean(torch.maximum(unclipped_loss, clipped_loss)) tensor(-1.0022, device='cuda:0')
line 385 loss tensor(-1.0022, device='cuda:0')
line 386fi loss.size() torch.Size([])
line 537 loss.requires_grad False
line 538 loss.shape torch.Size([])

Same problem
So I tried getting rid of the no gradient decorator
result:
  File "/home/jlb638/.conda/envs/clip/lib/python3.11/site-packages/trl/trainer/ddpo_trainer.py", line 242, in step
    rewards = self.accelerator.gather(rewards).cpu().numpy()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.

next step:
get rid of the .cpu().numpy() let that shit gather + torch.no_grad() rempved 
result: 
RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.

next step:
line 263 
+ advantages=torch.tensor(advantages, requires_grad=True)
and still torch.no_grad() removed
result:
    rewards = self.accelerator.gather(rewards).cpu().numpy()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.
srun: error: gpu018: task 0: Exited with exit code 1

next step:
line 263 
+ advantages=torch.tensor(advantages, requires_grad=True)
result:
RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.

next step:
get rid of the .cpu().numpy() let that shit gather
result:
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

change nothign but 
    sample_batch_size=2,
    train_batch_size=2,
result:
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
srun: error: gpu018: task 0: Exited with exit code 1

change nothign but 
    sample_batch_size=1,
    train_batch_size=2,
result:
ValueError: Sample batch size (1) must be greater than or equal to the train batch size (2)

try:
    sample_batch_size=2,
    train_batch_size=1,
result:
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

try:
+ advantages=torch.tensor(advantages, requires_grad=True)
results:
RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.
srun: error: gpu018: task 0: Exited with exit code 1

try:
try:
+ advantages=torch.tensor(advantages, requires_grad=True)
sample_num_steps=1
result: RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.

try:
+ advantages=torch.tensor(advantages, requires_grad=True)
sample_num_steps=1
+ self.accelerator.backward(loss,retain_graph=True)
THAT WORKED...I think...???

Problem, when running LORA wikiart100 ddpo, we get
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacty of 11.92 GiB of which 1.53 GiB is free. Including non-PyTorch memory, this process has 10.39 GiB memory in use. Of the allocated memory 7.94 GiB is allocated by PyTorch, and 2.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

Possible solution(s):
calling torch.cuda.empty_cache()
smaller batch size
train_gradient_accumulation_steps increase/decrease???
max_split_size_mb https://stackoverflow.com/questions/73747731/runtimeerror-cuda-out-of-memory-how-setting-max-split-size-mb 

try:
sbatch runpygpu.sh ddpo_train_script.py --pretrained_model_name_or_path "jlbaker361/sd-wikiart-lora-balanced100" --output_dir "/scratch/jlb638/sd-wiki100-ddpo" --hub_model_id "jlbaker361/sd-wiki100-ddpo" --train_gradient_accumulation_steps 8 --train_batch_size 1 --num_epochs 10 --sample_num_steps 20 --sample_batch_size 1  --sample_num_batches_per_epoch 64
result:
this might have made it worse (~60 images) but also smaller # of samples per batch so maybe the problem is just at the end of the epoch it shits itself??

try:
even bigger train_gradient_accumulation_steps

result:
nothing

try:
only 2 gpus? but more 128 RAM

result:
same issue

try:
train_gradient_accumulation_steps=samples per epoch=64

result:

CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacty of 23.69 GiB of which 954.81 MiB is free

try:
sample_num_batches_per_epoch = train_gradient_accumulation_steps=1

result:
CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacty of 11.92 GiB of which 205.19 MiB is free.
So the gpu has a lower capacity for some reason now???

try: sampling steps = 4

result:
same problem

theory:
different gpus have different sizes???

try:
test ddpo-test like 20 times and see if theres any variance

result:

observation:
doubled train batch size and sample batch size and allocated memory doubles

try:
train_batch_size=4 sample_batch_size=1

ValueError: Sample batch size (1) must be greater than or equal to the train batch size (4)

try:
train_batch_size=1 sample_batch_size=4

result:
CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacty of 11.92 GiB of which 3.43 GiB is free. Including non-PyTorch memory, this process has 8.48 GiB memory in use. Of the allocated memory 8.29 GiB is allocated by PyTorch, and 65.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

observation: there IS some variance based on the gpu being used

I do think it has to do with the gpu BUT there might be other ways to reduce memory

try:
using only one gpu

result:
same problem

try:
adding
gc.collect()
torch.cuda.empty_cache()
self.accelerator.free_memory()
before we do the self.accelerator.backward()

result:
same problem

try:
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:64

PROBLEM
ddpo_train_script.py --train_batch_size 8 --num_epochs 10 --sample_num_steps 20 --sample_batch_size 8 --sample_num_batches_per_epoch 64
IMMEDIATELY OOM- can't generate shit
job 33640873

try:
might be a fluke; just run again but with use_lora=False

result:
same problem

try: only 2 gpus not 4 ??? but 128 RAM

result: same issue

try: train lora for 0 epochs and then load that

result:
worked :)))

next step: try big gpu

result:
oof

problem:
Traceback (most recent call last):
  File "/home/ubuntu/clipcreate/ddpo_train_script.py", line 136, in <module>
    raise exc
  File "/home/ubuntu/clipcreate/ddpo_train_script.py", line 127, in <module>
    trainer.train()
  File "/home/ubuntu/miniconda3/envs/clip/lib/python3.11/site-packages/trl/trainer/ddpo_trainer.py", line 596, in train
    global_step = self.step(epoch, global_step)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/miniconda3/envs/clip/lib/python3.11/site-packages/trl/trainer/ddpo_trainer.py", line 260, in step
    self.image_samples_callback(prompt_image_data, global_step, self.accelerator.trackers[0])
                                                                ~~~~~~~~~~~~~~~~~~~~~~~~~^^^
IndexError: list index out of range

solution:
use ur own branch of trl diffusers

resnet training bus error- is this OOM related???

try: runpymain 256GB memory

result:
???

Problem:
RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor
srun: error: gpu006: task 0: Exited with exit code 1

solution:
noise.to(accelerator.device)

result:
nothing

solution:
dataloader for noise to pass to acleratoer.prepare ? ?


problems: graph getting broken or something

solution:
disc optimizer before generator???

result:
naur

solution:
call detach on fake output

result: didn't work BUT the variable in question was the discirimnator clssifier final layer
solutin: try detaching???

result:
RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.

solution:
retain_graph=True???

result:
no

current problem:
shits itself for when we do gen gradient

solution:
make fake_labels, fake_style TWICE, once with detach one without

result:
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [131072, 1]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient
this is the thing that does the classification

solution:
do all the discrikinator stuff THEN the generator stuff

result:
that worked lmfao :)))

problem:
OOM but it works!!!

solution:
retain_graph=False???

result:
that worked lmfao

problem:
slow as shit :(((

solution:
delete and restart... i suspect hf may be being stupid

result:
hf is being very stupid indeed

problem:
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

solution:
wrap tensors in???

cross entropy problem???
solution: try with cosine

1/28 1:44 PM- shitty clip reward function works!!!

1/28 2:08 PM
Normal CLIP reward works!

1/28 2:27 PM
DCGAN reward works maybe???

1/28 8:57 PM
CAN does not work... after one step it just gives up

solution:
> separate all the gradient updtes??? didnt seem to work...
> more layers in discriminator true or false

1/29 10:16 AM
OOM for creativity- smaller batch size???
yee

1/29 3:51 PM
ddpo train script not working- hardware prboleme? 

faulthandler:

Thread 0x00007f30b3ba5700 (most recent call first):
  File "/home/kasm-user/miniconda3/lib/python3.11/threading.py", line 324 in wait
  File "/home/kasm-user/miniconda3/lib/python3.11/threading.py", line 622 in wait
  File "/home/kasm-user/miniconda3/lib/python3.11/site-packages/tqdm/_monitor.py", line 60 in run
  File "/home/kasm-user/miniconda3/lib/python3.11/threading.py", line 1038 in _bootstrap_inner
  File "/home/kasm-user/miniconda3/lib/python3.11/threading.py", line 995 in _bootstrap

Thread 0x00007f3194ffd700 (most recent call first):
  File "/home/kasm-user/miniconda3/lib/python3.11/threading.py", line 324 in wait
  File "/home/kasm-user/miniconda3/lib/python3.11/threading.py", line 622 in wait
  File "/home/kasm-user/miniconda3/lib/python3.11/site-packages/tqdm/_monitor.py", line 60 in run
  File "/home/kasm-user/miniconda3/lib/python3.11/threading.py", line 1038 in _bootstrap_inner
  File "/home/kasm-user/miniconda3/lib/python3.11/threading.py", line 995 in _bootstrap

Current thread 0x00007f351619e4c0 (most recent call first):
  File "/home/kasm-user/miniconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 456 in _conv_forward
  File "/home/kasm-user/miniconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 460 in forward
  File "/home/kasm-user/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520 in _call_impl
  File "/home/kasm-user/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511 in _wrapped_call_impl
  File "/home/kasm-user/miniconda3/lib/python3.11/site-packages/diffusers/models/unet_2d_condition.py", line 1072 in forward
  File "/home/kasm-user/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520 in _call_impl
  File "/home/kasm-user/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511 in _wrapped_call_impl
  File "/home/kasm-user/miniconda3/lib/python3.11/site-packages/trl/models/modeling_sd_base.py", line 479 in pipeline_step
  File "/home/kasm-user/miniconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115 in decorate_context
  File "/home/kasm-user/miniconda3/lib/python3.11/site-packages/trl/models/modeling_sd_base.py", line 564 in __call__
  File "/home/kasm-user/miniconda3/lib/python3.11/site-packages/trl/trainer/ddpo_trainer.py", line 476 in _generate_samples
  File "/home/kasm-user/miniconda3/lib/python3.11/site-packages/trl/trainer/ddpo_trainer.py", line 251 in step
  File "/home/kasm-user/miniconda3/lib/python3.11/site-packages/trl/trainer/ddpo_trainer.py", line 604 in train
  File "/home/kasm-user/clipcreate/ddpo_train_script.py", line 202 in <module>

Extension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, torch._C, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, _brotli, yaml._yaml, PIL._imaging, regex._regex, zstandard.backend_c, pyarrow.lib, pyarrow._hdfsio, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, pyarrow._parquet, pyarrow._fs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, aiohttp._helpers, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, PIL._imagingft, scipy._lib._ccallback_c, numpy.linalg.lapack_lite, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._flinalg, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.io.matlab._mio_utils, scipy.io.matlab._streams, scipy.io.matlab._mio5_utils, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.optimize._minpack2, scipy.optimize._group_columns, scipy._lib.messagestream, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize._highs.cython.src._highs_wrapper, scipy.optimize._highs._highs_wrapper, scipy.optimize._highs.cython.src._highs_constants, scipy.optimize._highs._highs_constants, scipy.linalg._interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.spatial._ckdtree, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.spatial.transform._rotation, scipy.optimize._direct (total: 150)
ddpo-test.sh: line 2:  7445 Segmentation fault      (core dumped) python3 ddpo_train_script.py --image_dir "~/elgammal-0-images/" --output_dir "elgammal-0-ddpo-2" --hub_model_id "jlbaker361/elgammal-0-ddpo-2" --train_gradient_accumulation_steps 1 --train_batch_size 4 --num_epochs 2 --sample_num_steps 10 --sample_batch_size 4 --sample_num_batches_per_epoch 8 --resume_from "~/clipcreate/elgammal-0-ddpo-2/checkpoints" --dataset_name "jlbaker361/wikiart-balanced1000" --reward_function dcgan --dcgan_repo_id "jlbaker361/dcgan-wikiart500-resized"

solution:
use conda

conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia
conda install -c conda-forge wandb huggingface_hub peft diffusers bitsandbytes scipy matplotlib trl datasets

problem: can't load lora weights???
like the adapter doesn't recognize the weight names
when use_lora=True
stability base weight names:
down_blocks.0.attentions.0.transformer_blocks.0.norm1.bias
down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight
down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight
down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight
down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight
down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias
down_blocks.0.attentions.0.transformer_blocks.0.norm2.weight

stability weight names AFTER trying to load lora weights:

down_blocks.0.attentions.0.transformer_blocks.0.norm1.bias
down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.base_layer.weight
down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default_0.weight
down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.default_0.weight
down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.base_layer.weight
down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.default_0.weight
down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.default_0.weight
down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.base_layer.weight
down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.default_0.weight
down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.default_0.weight
down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.base_layer.weight
down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.base_layer.bias
down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_A.default_0.weight
down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_B.default_0.weight

lora weight names:

 'down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.lora.up.weight',
 'down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.lora.up_1.weight',
 'down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.lora.down.weight',
 'down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.lora.down_1.weight',
 'down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.lora.up.weight',
 'down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.lora.up_1.weight',
 'down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.lora.down.weight',
 'down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.lora.down_1.weight',
 'down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.lora.up.weight',

 lets try to train ddpo-stability-test for a second and load the lora weights???
 so we dont get the :((( message

 YES if you train it again the new pytorch weights trigger the error

syability weight names:
down_blocks.0.attentions.0.transformer_blocks.0.norm1.weight
down_blocks.0.attentions.0.transformer_blocks.0.norm1.bias
down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight
down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight
down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight
down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight
down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias
down_blocks.0.attentions.0.transformer_blocks.0.norm2.weight
down_blocks.0.attentions.0.transformer_blocks.0.norm2.bias
down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight
down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight
down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight
down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weigh

stability after loading weight names:

down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.base_layer.weight
down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default_0.weight
down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.default_0.weight
down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.base_layer.weight
down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.default_0.weight
down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.default_0.weight
down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.base_layer.weight
down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.default_0.weight
down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.default_0.weight
down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.base_layer.weight
down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.base_layer.bias
down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_A.default_0.weight

lora weights:

unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.lora.up_1.weight',
 'unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.lora.down.weight',
 'unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.lora.down_1.weight',
 'unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.lora.up.weight',
 'unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.lora.up_1.weight',
 'unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.lora.down.weight',
 'unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.lora.down_1.weight',
 'unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.lora.up.weight',
 'unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.lora.up_1.weight',
 'unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.lora.down.weight',
 'unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.lora.down_1.weight',
 'unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.lora.up.weight',
 'unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.lora.up_1.weight',
 'unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.lora.down.weight',
 'unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.lora.down_1.weight',
 'unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.lora.up.weight',
 'unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.lora.up_1.weight',

this was the last snapshot that worked:
ab8dc5de3833bd2604988f3e7df16dcdb63c1a08

so the tldr is once it loads from a "second hand" lora and tries to retrain it starts to shit itself

ddpo-stability-good is our good model

