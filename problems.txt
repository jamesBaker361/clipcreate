slow_conv2d_cpu" not implemented for 'Half'
when dtype=float16
apparently this is caused by not using GPU:
https://stackoverflow.com/questions/74725439/runtimeerror-slow-conv2d-cpu-not-implemented-for-half
solution: do BLIP but not with fp16

Dec 12
RuntimeError: "LayerNormKernelImpl" not implemented for 'Half'

possibly caused by use mixed precision when it should be full precision

problem:
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

solution:
might be a problem with adamW optimizer
/home/jlb638/.conda/envs/clip/lib/python3.11/site-packages/trl/trainer/ddpo_trainer.py L398 
change to adam optimizer

12/13

DDPO with creativity: 

['ddpo_test.py', 'no']
images.shape  torch.Size([1, 3, 512, 512])
images.shape  torch.Size([1, 3, 512, 512])
line 233 rewards  (tensor([-0.9994], device='cuda:0'), tensor([-0.9993], device='cuda:0'))
line 244 rewards  [-0.9993682 -0.9993267]
line 263 advantages  [-0.9995181  0.9995181]
line 272 samples["advantages"] tensor([-0.9995,  0.9995], device='cuda:0')
line 273 samples["advantages"].requires_grad False
line 521 sample["advantages"] tensor([0.9995], device='cuda:0')
line 343 advantages False torch.Size([1])
line 377 advantages False torch.Size([1])
line 397 advantages.requires_grad False
line 398 advantages.size() torch.Size([1])
line 385 loss.requires_grad False
line 386 loss.size() torch.Size([])
line 537 loss.requires_grad False
line 538 loss.shape torch.Size([])

DDPO with aesthetic:
['ddpo_test.py', 'no', 'aesthetic']
line 233 rewards  (tensor([6.6416], device='cuda:0'), tensor([4.2934], device='cuda:0'))
line 244 rewards  [6.6416025 4.293413 ]
line 263 advantages  [ 1. -1.]
line 272 samples["advantages"] tensor([ 1., -1.], device='cuda:0')
line 273 samples["advantages"].requires_grad False
line 521 sample["advantages"] tensor([-1.], device='cuda:0')
line 343 advantages False torch.Size([1])
line 377 advantages False torch.Size([1])
line 397 advantages.requires_grad False
line 398 advantages.size() torch.Size([1])
line 385 loss.requires_grad False
line 386 loss.size() torch.Size([])
line 537 loss.requires_grad False
line 538 loss.shape torch.Size([])

maybe try running it on ONE GPU???

Soluton:
unsqueeze line 412???

images.shape  torch.Size([1, 3, 512, 512])
images.shape  torch.Size([1, 3, 512, 512])
line 233 rewards  (tensor([-9.9937], device='cuda:0'), tensor([-9.9933], device='cuda:0'))
line 244 rewards  [-9.993682 -9.993267]
line 263 advantages  [-0.9976504  1.0022479]
line 272 samples["advantages"] tensor([-0.9977,  1.0022], device='cuda:0')
line 273 samples["advantages"].requires_grad False
line 521 sample["advantages"] tensor([1.0022], device='cuda:0')
line 343 advantages False torch.Size([1])
line 377 advantages False torch.Size([1])
line 400 advantages.requires_grad False
line 401 advantages.size() torch.Size([1])
line 408 unclipped_loss  tensor([-1.0022], device='cuda:0')
line 409 clipped_loss tensor([-1.0022], device='cuda:0')
line 410 torch.maximum(unclipped_loss, clipped_loss) tensor([-1.0022], device='cuda:0')
line 411 torch.mean(torch.maximum(unclipped_loss, clipped_loss)) tensor(-1.0022, device='cuda:0')
line 385 loss tensor(-1.0022, device='cuda:0')
line 386fi loss.size() torch.Size([])
line 537 loss.requires_grad False
line 538 loss.shape torch.Size([])

Same problem
So I tried getting rid of the no gradient decorator
result:
  File "/home/jlb638/.conda/envs/clip/lib/python3.11/site-packages/trl/trainer/ddpo_trainer.py", line 242, in step
    rewards = self.accelerator.gather(rewards).cpu().numpy()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.

next step:
get rid of the .cpu().numpy() let that shit gather + torch.no_grad() rempved 
result: 
RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.

next step:
line 263 
+ advantages=torch.tensor(advantages, requires_grad=True)
and still torch.no_grad() removed
result:
    rewards = self.accelerator.gather(rewards).cpu().numpy()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.
srun: error: gpu018: task 0: Exited with exit code 1

next step:
line 263 
+ advantages=torch.tensor(advantages, requires_grad=True)
result:
RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.

next step:
get rid of the .cpu().numpy() let that shit gather
result:
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

change nothign but 
    sample_batch_size=2,
    train_batch_size=2,
result:
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
srun: error: gpu018: task 0: Exited with exit code 1

change nothign but 
    sample_batch_size=1,
    train_batch_size=2,
result:
ValueError: Sample batch size (1) must be greater than or equal to the train batch size (2)

try:
    sample_batch_size=2,
    train_batch_size=1,
result:
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

try:
+ advantages=torch.tensor(advantages, requires_grad=True)
results:
RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.
srun: error: gpu018: task 0: Exited with exit code 1

try:
try:
+ advantages=torch.tensor(advantages, requires_grad=True)
sample_num_steps=1
result: RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.

try:
+ advantages=torch.tensor(advantages, requires_grad=True)
sample_num_steps=1
+ self.accelerator.backward(loss,retain_graph=True)
THAT WORKED...I think...???

Problem, when running LORA wikiart100 ddpo, we get
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacty of 11.92 GiB of which 1.53 GiB is free. Including non-PyTorch memory, this process has 10.39 GiB memory in use. Of the allocated memory 7.94 GiB is allocated by PyTorch, and 2.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

Possible solution(s):
calling torch.cuda.empty_cache()
smaller batch size
train_gradient_accumulation_steps increase/decrease???
max_split_size_mb https://stackoverflow.com/questions/73747731/runtimeerror-cuda-out-of-memory-how-setting-max-split-size-mb 

try:
sbatch runpygpu.sh ddpo_train_script.py --pretrained_model_name_or_path "jlbaker361/sd-wikiart-lora-balanced100" --output_dir "/scratch/jlb638/sd-wiki100-ddpo" --hub_model_id "jlbaker361/sd-wiki100-ddpo" --train_gradient_accumulation_steps 8 --train_batch_size 1 --num_epochs 10 --sample_num_steps 20 --sample_batch_size 1  --sample_num_batches_per_epoch 64
result:
this might have made it worse (~60 images) but also smaller # of samples per batch so maybe the problem is just at the end of the epoch it shits itself??

try:
even bigger train_gradient_accumulation_steps

result:
nothing

try:
only 2 gpus? but more 128 RAM

result:
same issue

try:
train_gradient_accumulation_steps=samples per epoch=64

result:

CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacty of 23.69 GiB of which 954.81 MiB is free

try:
sample_num_batches_per_epoch = train_gradient_accumulation_steps=1

result:
CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacty of 11.92 GiB of which 205.19 MiB is free.
So the gpu has a lower capacity for some reason now???

try: sampling steps = 4

result:
same problem

theory:
different gpus have different sizes???

try:
test ddpo-test like 20 times and see if theres any variance

result:

observation:
doubled train batch size and sample batch size and allocated memory doubles

try:
train_batch_size=4 sample_batch_size=1

ValueError: Sample batch size (1) must be greater than or equal to the train batch size (4)

try:
train_batch_size=1 sample_batch_size=4

result:
CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacty of 11.92 GiB of which 3.43 GiB is free. Including non-PyTorch memory, this process has 8.48 GiB memory in use. Of the allocated memory 8.29 GiB is allocated by PyTorch, and 65.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

observation: there IS some variance based on the gpu being used

I do think it has to do with the gpu BUT there might be other ways to reduce memory

try:
using only one gpu

result:
same problem

try:
adding
gc.collect()
torch.cuda.empty_cache()
self.accelerator.free_memory()
before we do the self.accelerator.backward()

result:
same problem

try:
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:64

PROBLEM
ddpo_train_script.py --train_batch_size 8 --num_epochs 10 --sample_num_steps 20 --sample_batch_size 8 --sample_num_batches_per_epoch 64
IMMEDIATELY OOM- can't generate shit
job 33640873

try:
might be a fluke; just run again but with use_lora=False

result:
same problem

try: only 2 gpus not 4 ??? but 128 RAM

result: same issue

try: train lora for 0 epochs and then load that

result:
worked :)))

next step: try big gpu

result:
oof

problem:
Traceback (most recent call last):
  File "/home/ubuntu/clipcreate/ddpo_train_script.py", line 136, in <module>
    raise exc
  File "/home/ubuntu/clipcreate/ddpo_train_script.py", line 127, in <module>
    trainer.train()
  File "/home/ubuntu/miniconda3/envs/clip/lib/python3.11/site-packages/trl/trainer/ddpo_trainer.py", line 596, in train
    global_step = self.step(epoch, global_step)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/miniconda3/envs/clip/lib/python3.11/site-packages/trl/trainer/ddpo_trainer.py", line 260, in step
    self.image_samples_callback(prompt_image_data, global_step, self.accelerator.trackers[0])
                                                                ~~~~~~~~~~~~~~~~~~~~~~~~~^^^
IndexError: list index out of range

solution:
use ur own branch of trl diffusers